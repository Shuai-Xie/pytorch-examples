{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.einsum() ` 爱因斯坦求和约定\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/44954540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3],\n",
       "        [1, 4],\n",
       "        [2, 5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转置\n",
    "b = torch.einsum('ij->ji', [a])  # 不会改变 a 的值\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求和\n",
    "torch.einsum('ij->', [a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 沿列求和，消去 j\n",
    "torch.einsum('ij->i', [a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 5, 7])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 沿行求和，消去 i\n",
    "torch.einsum('ij->j', [a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 25,  28,  31,  34,  37],\n",
       "        [ 70,  82,  94, 106, 118]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵相乘\n",
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(15).reshape(3, 5)\n",
    "torch.einsum('ij,jk->ik', [a, b])  # -> 左侧代表了 参与计算的元素个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1020, 1175, 1330],\n",
       "        [3180, 3650, 4120]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(15).reshape(3, 5)\n",
    "c = torch.arange(15).reshape(5, 3)\n",
    "torch.einsum('ij,jk,km->im', [a, b, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14)\n",
      "tensor([ 0,  4, 10])\n"
     ]
    }
   ],
   "source": [
    "# 向量 点积 数量积 标量积，得到实数\n",
    "a = torch.arange(3)\n",
    "b = torch.arange(3, 6)\n",
    "res = torch.einsum('i,i->', [a,b])\n",
    "print(res)\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(145)\n",
      "tensor(145)\n"
     ]
    }
   ],
   "source": [
    "# 矩阵点积 对应元素相乘求和\n",
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(6,12).reshape(2, 3)\n",
    "res = torch.einsum('ij,ij->', [a, b])\n",
    "print(res)\n",
    "print(torch.sum(a*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  7, 16],\n",
      "        [27, 40, 55]])\n",
      "tensor([[ 0,  7, 16],\n",
      "        [27, 40, 55]])\n"
     ]
    }
   ],
   "source": [
    "# hadamard 哈达玛积 element-wise product\n",
    "print(torch.einsum('ij,ij->ij', [a,b]))\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n",
      "tensor([3, 4, 5, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0],\n",
       "        [ 3,  4,  5,  6],\n",
       "        [ 6,  8, 10, 12]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 外积\n",
    "a = torch.arange(3)\n",
    "b = torch.arange(3,7)\n",
    "print(a)\n",
    "print(b)\n",
    "torch.einsum('i,j->ij', [a, b])  # 向量 -> 矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0],\n",
       "        [ 3,  4,  5,  6],\n",
       "        [ 6,  8, 10, 12]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵 * 行向量\n",
    "a.reshape(3,1).repeat(1,4) * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1521,  0.6876,  0.2198],\n",
       "         [-1.3261, -2.3821,  1.9830]],\n",
       "\n",
       "        [[-1.7016, -1.2594, -5.2988],\n",
       "         [-3.9042, -2.7346,  0.5616]],\n",
       "\n",
       "        [[-0.4510,  2.4994, -4.0824],\n",
       "         [ 5.6344,  6.4358, -1.0691]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch 矩阵相乘\n",
    "a = torch.randn(3,2,5)\n",
    "b = torch.randn(3,5,3)\n",
    "# jk, kl -> jl, i 作为 batch 不变\n",
    "torch.einsum('ijk,ikl->ijl', [a, b])  # 3,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 11, 13, 17])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量缩约\n",
    "a = torch.randn(2,3,5,7)\n",
    "b = torch.randn(11,13,3,17,5)\n",
    "torch.einsum('pqrs,tuqvr->pstuv', [a, b]).shape  # 3,5 相等的 dim 约去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 10, 3\n",
    "\n",
    "a = torch.randn(n, p)\n",
    "print(torch.cov)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
